using NEIS
using LinearAlgebra, Printf, Random
using Statistics
using JLD2, FileIO
using Plots
using Plots.PlotMeasures
using LaTeXStrings
plot_setup()
push!(LOAD_PATH,"./")
using VisualizeComparison
setup_folder()

to_train = true
to_plot = true
to_compare = true
to_compute_data = true
xmin = -10; xmax = 4; ymin = -7; ymax = 7;

########################################
# Load the model
testcasenum = 3
n = 10
# n = 2
œÉf = 3.0
U‚ÇÄ, U‚ÇÅ, exact_mean, reduce_percent = load_eg3(n, œÉf)
T = Float64

########################################
# training parameters
N = 100
discretize_method = "int"
tmname = "sym"; tm = -1/2;
offset = estimate_offset(tm, N)

########################################
# ansatz
scale = 3.0

for model_num in [0, 4]
    println("&"^40)

    if model_num == 0
        ‚Ñì = 1; m_list = [n]; # linear ùêõ(x) = A x + b
        seed_list = [2, 3, 4]
        numsample_max = 10^3
        biased = true
        train_step = 200
        lr = 0.3
        œµ = 0.2
        decay = (1/œµ - 1)/train_step
    elseif model_num == 4
        ‚Ñì = 1; m_list = [0] # two-parameter form
        seed_list = [1]
        numsample_max = 10^3
        biased = false
        train_step = 100
        lr = 0.3
        œµ = 0.5
        decay = (1/œµ - 1)/train_step
    end
    model_list = [model_num]

    ########################################
    # whether to use assisted training
    if biased
        biased_percent = 0.3; biased_scale = 1.0; œÖ = 0.7;
        biased_func = (x,i) -> biased_GD_func(x, i,
                    train_step, biased_percent, U‚ÇÅ, N,
                    s = biased_scale, œÖ = œÖ)
    else
        biased_percent = 0.0; biased_scale = 1.0; œÖ = 0.0
        biased_func = nothing
    end

    ########################################
    # more training parameters
    train_paras = Dict{Symbol, Any}(:ref_value => exact_mean,
                :verbose => false,
                :max_search_iter => 0,
                :numsample_min => numsample_max,
                :biased_func => biased_func,
                :printpara => false,
                :quiet => false)
    if model_num == 0
        train_paras[:savepara] = false
    elseif model_num == 4
        train_paras[:savepara] = true
    end

    ########################################
    # loss function energy landscape
    train_paras[:œïname] = "msq"; train_paras[:œïœµ] = 1.0e-4;
    œï = get_Œ¶(train_paras[:œïname], train_paras[:œïœµ])

    ModelParas = Dict("testcasenum"=>testcasenum, "n"=>n, "N"=>N, "numsample_max"=>numsample_max,
                      "‚Ñì"=>‚Ñì, "model_num"=>0, "m"=>0, "œï"=>œï)
    data_folder = "./data/"

    œÜ = x -> log10.(x)
    #œÜ = x -> x
    Z‚ÇÅ  = sqrt(2*œÄ)^n*œÉf*reduce_percent
    œÅ‚ÇÅ(x,y) = œÜ(exp(-U‚ÇÅ(vcat([x,y],zeros(n-2))))/Z‚ÇÅ)

    ########################################
    # start training
    if to_train
        for (seed, m) in Iterators.product(seed_list, m_list)
            separate_line_small()
            @printf("model number = %1d, œï = %s, m = %3d, seed = %2d\n", model_num, œï.name, m, seed)

            Random.seed!(seed)
            ModelParas["m"] = m; ModelParas["model_num"] = model_num
            filename = get_experiment_name(ModelParas, seed, data_folder)

            # use a optimized implementation for better speed
            flow = init_rand_dyn(model_num, n, m, scale, U‚ÇÅ)

            # do the training (statistics will be reset), data will be saved automatically.
            train_flow(U‚ÇÄ, U‚ÇÅ, flow, N, numsample_max, train_step, lr, decay,
                       discretize_method, offset, train_paras, filename=filename, save_data=true)
        end
    end

    ########################################
    # show results
    if to_plot
        show_training_result_various_ansatz_and_m(model_list, m_list, seed_list,
              data_folder, ModelParas, exact_mean, œÖ, train_step, "eg"*string(testcasenum))

        if model_num == 4
            ModelParas["model_num"] = model_num
            ModelParas["m"] = m_list[1]
            seed = seed_list[1]
            filename = get_experiment_name(ModelParas, seed, data_folder)
            est_para = load(filename, "est_para")
            f1, max_v, min_v = plot_error_or_divg(seed_list, data_folder, ModelParas, exact_mean,
                                                  fig_type=:err, title="error", nolabel=true, margin=20px)
            f2, max_v, min_v = plot_error_or_divg(seed_list, data_folder, ModelParas, exact_mean,
                                                  fig_type=:divg, title="var", nolabel=true, margin=20px)
            Œ≤list = [item[1][1] for item in est_para]
            Œ±list = [item[1][2] for item in est_para]
            f3 = plot(1:train_step, Œ≤list, label=L"\beta", title="parameters")
            plot!(1:train_step, Œ±list, label=L"\alpha", margin=20px)
            fig_train = plot(f1, f2, f3, layout=(@layout grid(1,3)), size=(800,250))
            fn = @sprintf("assets/train_case_%d_model_%d_phi_%s_with_para.pdf", ModelParas["testcasenum"],
                                                                             ModelParas["model_num"],
                                                                             ModelParas["œï"].name)
            savefig(fig_train, fn)
        end
    end

    ########################################
    # Compare with AIS
    if to_compare
        print_current_resource()
        println("="^40)
        printstyled("Start comparison\n", bold=true, color=:blue)
        println("="^40)

        percent_train = 1/7; numrepeat = 10;
        solver = RK4
        loc_y = 1.35; gap = 0.1; ylim = (0.5, 1.35); outliers = false

        for (m, model_num) in Iterators.product(m_list, model_list)
            printstyled(@sprintf("Comparison for model_num=%d, m=%d\n", model_num, m),
                    bold=true, color=:blue)
            ModelParas["m"] = m; ModelParas["model_num"] = model_num
            show_comparison_for_all_random_initializations(U‚ÇÄ, U‚ÇÅ, ModelParas,
                                        seed_list, data_folder,
                                        xmin, xmax, ymin, ymax,
                                        solver, discretize_method, offset, exact_mean,
                                        numrepeat, percent_train, to_compute_data,
                                        loc_y=loc_y, gap=gap, ylim=ylim, outliers=outliers, rho=œÅ‚ÇÅ)
        end
    end # end of comparison

    print_data(m_list, model_list, seed_list, ModelParas, data_folder)

    println("&"^40)
end # end of model number
